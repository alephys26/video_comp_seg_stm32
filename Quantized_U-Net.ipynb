{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1132317,"sourceType":"datasetVersion","datasetId":635428}],"dockerImageVersionId":30684,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\n!pip install torchsummary\nfrom torchsummary import summary\nimport torch.nn as nn\nfrom torchvision import datasets, transforms\nfrom torchvision.transforms import ToTensor\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nimport torch.nn.functional as F\nfrom torchvision.transforms import ToTensor\nfrom torch.utils.data import DataLoader ,  Dataset\nfrom PIL import Image\nimport os","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-25T13:21:30.951284Z","iopub.execute_input":"2024-04-25T13:21:30.951707Z","iopub.status.idle":"2024-04-25T13:21:52.048993Z","shell.execute_reply.started":"2024-04-25T13:21:30.951676Z","shell.execute_reply":"2024-04-25T13:21:52.047574Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"markdown","source":"## U-Net Model","metadata":{}},{"cell_type":"code","source":"class UNet(nn.Module):\n    def __init__(self, n_class = 2):\n        super(UNet,self).__init__()\n        \n        self.quant = torch.ao.quantization.QuantStub()\n        self.dequant = torch.ao.quantization.DeQuantStub()\n        \n        self.conv_block_down1 = nn.Sequential(\n            nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, padding=1, stride=1, bias=False),\n            nn.ReLU(inplace = True),\n            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1, stride=1, bias=False),\n            nn.ReLU(inplace = True),\n        )\n        \n        self.downsample = nn.Sequential(\n            nn.MaxPool2d(4,4)\n        )\n        \n        self.conv_block_down2 = nn.Sequential(\n            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1, stride=1, bias=False),\n            nn.ReLU(inplace = True),\n            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1, stride=1, bias=False),\n            nn.ReLU(inplace = True),\n        )\n        \n        self.conv_block_up1 = nn.Sequential(\n            nn.Conv2d(in_channels=128+64, out_channels=64, kernel_size=3, padding=1, stride=1, bias=False),\n            nn.ReLU(inplace = True),\n            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1, stride=1, bias=False),\n            nn.ReLU(inplace = True),\n        )\n        \n        \n        self.upsample = nn.Upsample(scale_factor=4, mode='bilinear', align_corners=True) \n        \n        self.conv_last = nn.Conv2d(64, n_class, 1, bias=False)\n        \n    def forward(self,x):\n        \n        x = self.quant(x)\n        \n        conv1 = self.conv_block_down1(x)\n        x = self.downsample(conv1)\n        \n        x = self.conv_block_down2(x)\n        \n        x = self.upsample(x)\n        x = torch.cat([x, conv1], dim=1)\n        \n        x = self.conv_block_up1(x)\n        \n        out = self.conv_last(x)\n        \n        return out\n    \n    \nmodel = UNet()\n\nsample_input = ( 3, 16, 16)\n\nsummary(model, sample_input)","metadata":{"execution":{"iopub.status.busy":"2024-04-23T12:27:46.440491Z","iopub.execute_input":"2024-04-23T12:27:46.440907Z","iopub.status.idle":"2024-04-23T12:27:46.796596Z","shell.execute_reply.started":"2024-04-23T12:27:46.440878Z","shell.execute_reply":"2024-04-23T12:27:46.795424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## U-Net model with single convolution layer per block","metadata":{}},{"cell_type":"code","source":"class UNet(nn.Module):\n    def __init__(self, n_class = 2):\n        super(UNet,self).__init__()\n        \n        self.quant = torch.ao.quantization.QuantStub()\n        self.dequant = torch.ao.quantization.DeQuantStub()\n        \n        self.conv_block_down1 = nn.Sequential(\n            nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, padding=1, stride=1, bias=False),\n            nn.ReLU(inplace = True),\n        )\n        \n        self.downsample = nn.Sequential(\n            nn.MaxPool2d(4,4)\n        )\n        \n        self.conv_block_down2 = nn.Sequential(\n            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1, stride=1, bias=False),\n            nn.ReLU(inplace = True),\n        )\n        \n        self.conv_block_down3 = nn.Sequential(\n            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1, stride=1, bias=False),\n            nn.ReLU(inplace = True),\n        )\n        \n        self.conv_block_up2 = nn.Sequential(\n            nn.Conv2d(in_channels=128+64, out_channels=64, kernel_size=3, padding=1, stride=1, bias=False),\n            nn.ReLU(inplace = True),\n        )\n        \n        self.conv_block_up1 = nn.Sequential(\n            nn.Conv2d(in_channels=256+128, out_channels=128, kernel_size=3, padding=1, stride=1, bias=False),\n            nn.ReLU(inplace = True),\n        )\n        \n        \n        self.upsample = nn.Upsample(scale_factor=4, mode='bilinear', align_corners=True) \n        \n        self.conv_last = nn.Conv2d(64, n_class, 1)\n        \n    def forward(self,x):\n        \n        x = self.quant(x)\n        \n        conv1 = self.conv_block_down1(x)\n        x = self.downsample(conv1)\n        \n        conv2 = self.conv_block_down2(x)\n        x = self.downsample(conv2)\n        \n        x = self.conv_block_down3(x)\n        \n        x = self.upsample(x)\n        x = torch.cat([x, conv2], dim=1)\n        \n        x = self.conv_block_up1(x)\n        \n        x = self.upsample(x)\n        x = torch.cat([x, conv1], dim=1)\n        \n        x = self.conv_block_up2(x)\n        \n        out = self.conv_last(x)\n        \n        return out\n    \n    \nmodel = UNet()\n\nsample_input = ( 3, 16, 16)\n\nsummary(model, sample_input)","metadata":{"execution":{"iopub.status.busy":"2024-04-25T13:22:47.056898Z","iopub.execute_input":"2024-04-25T13:22:47.057552Z","iopub.status.idle":"2024-04-25T13:22:47.438924Z","shell.execute_reply.started":"2024-04-25T13:22:47.057519Z","shell.execute_reply":"2024-04-25T13:22:47.437958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset","metadata":{}},{"cell_type":"code","source":"class CamVidDataset(Dataset):\n    def __init__(self, img_dir, mask_dir, class_dict, transform=None, mask_transform = None):\n        self.img_dir = img_dir\n        self.mask_dir = mask_dir\n        self.transform = transform\n        self.mask_transform = mask_transform\n        \n        self.class_dict = pd.read_csv(class_dict)\n\n        self.class_mapping = self._create_class_mapping()\n        \n        self.img_filenames = sorted(os.listdir(img_dir))\n        self.mask_filenames = sorted(os.listdir(mask_dir))\n        \n        \n        assert len(self.img_filenames) == len(self.mask_filenames), \"Mismatch in number of images and masks\"\n    \n    def _create_class_mapping(self):\n\n        class_mapping = {}\n\n        class_name_to_int = {}\n\n        # Assign unique integers to class labels\n        class_idx = 0\n        for _, row in self.class_dict.iterrows():\n            rgb = tuple(row[['r', 'g', 'b']])\n            class_name = row['name']\n\n            # If the class name is not already in the mapping, add it\n            if class_name not in class_name_to_int:\n                class_name_to_int[class_name] = class_idx\n                class_idx += 1\n\n            # Map RGB values to the integer class label\n            class_mapping[rgb] = class_name_to_int[class_name]\n\n        self.class_name_to_int = class_name_to_int\n        \n        return class_mapping\n\n        \n    def __len__(self):\n        return len(self.img_filenames)\n\n    def __getitem__(self, idx):\n\n        img_filename = self.img_filenames[idx]\n        mask_filename = self.mask_filenames[idx]\n        \n        img_path = os.path.join(self.img_dir, img_filename)\n        mask_path = os.path.join(self.mask_dir, mask_filename)\n        \n        image = Image.open(img_path).convert('RGB')\n        mask = Image.open(mask_path).convert('RGB')\n        \n        if self.transform:\n            image = self.transform(image)\n\n        # Convert mask from RGB to class labels\n        mask = self._convert_mask(mask)\n        \n        mask = mask.view(-1,mask.shape[0],mask.shape[1])\n        \n        if self.mask_transform:\n            mask = self.mask_transform(mask)\n        \n        return image, mask\n    \n    \n    def _convert_mask(self, mask):\n        \n        \n        # Convert RGB tuples to tensors for vectorized comparison\n        rgb_values = torch.tensor(list(self.class_mapping.keys())).float()\n\n        mask_np = np.array(mask)\n        mask_tensor = torch.from_numpy(mask_np)\n        \n\n        mask_tensor_flat = mask_tensor.view(-1, 3).float()\n\n        height, width, _ = mask_tensor.shape\n        class_mask = torch.zeros((height * width), dtype=torch.int64)\n\n        \n        # Compare the mask_tensor with each RGB tuple in class_mapping\n        for i, rgb_value in enumerate(rgb_values):\n\n            match = torch.all(mask_tensor_flat == rgb_value, dim=1)\n\n            # Flatten indices\n            indices = match.nonzero(as_tuple=True)[0]\n\n            # Assign class label to the corresponding pixels in class_mask\n            class_mask[indices] = list(self.class_mapping.values())[i]\n            \n        # Reshape the flattened class_mask tensor back to (height, width)\n        class_mask = class_mask.view(height, width)\n\n        return class_mask\n\n","metadata":{"execution":{"iopub.status.busy":"2024-04-21T10:34:34.801690Z","iopub.execute_input":"2024-04-21T10:34:34.802045Z","iopub.status.idle":"2024-04-21T10:34:34.819455Z","shell.execute_reply.started":"2024-04-21T10:34:34.802016Z","shell.execute_reply":"2024-04-21T10:34:34.818372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transform = transforms.Compose([\n    transforms.Resize((16, 16), antialias=True),\n    transforms.ToTensor()\n])\nmask_transform = transforms.Compose([\n    transforms.Resize((16, 16), antialias=True)\n])","metadata":{"execution":{"iopub.status.busy":"2024-04-21T10:34:37.502249Z","iopub.execute_input":"2024-04-21T10:34:37.502883Z","iopub.status.idle":"2024-04-21T10:34:37.508244Z","shell.execute_reply.started":"2024-04-21T10:34:37.502851Z","shell.execute_reply":"2024-04-21T10:34:37.507184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train","metadata":{}},{"cell_type":"code","source":"img_dir = '/kaggle/input/camvid/CamVid/train'\nmask_dir = '/kaggle/input/camvid/CamVid/train_labels'\nclass_dict_path = '/kaggle/input/camvid/CamVid/class_dict.csv'\n\ntrain_dataset  = CamVidDataset(img_dir, mask_dir, class_dict_path, transform=transform, mask_transform = mask_transform)\n\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-21T10:34:54.197410Z","iopub.execute_input":"2024-04-21T10:34:54.198207Z","iopub.status.idle":"2024-04-21T10:34:54.226573Z","shell.execute_reply.started":"2024-04-21T10:34:54.198173Z","shell.execute_reply":"2024-04-21T10:34:54.225840Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_loader_iterator = iter(train_loader)\n\nimages, masks = next(train_loader_iterator)\n\nprint(\"Image batch shape:\", images.shape)\nprint(\"Mask batch shape:\", masks.shape)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-21T10:34:56.048054Z","iopub.execute_input":"2024-04-21T10:34:56.048892Z","iopub.status.idle":"2024-04-21T10:35:01.043233Z","shell.execute_reply.started":"2024-04-21T10:34:56.048860Z","shell.execute_reply":"2024-04-21T10:35:01.042262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_loader_iterator = iter(train_loader)\nimages, masks = next(train_loader_iterator)\n\nplt.figure(figsize=(10, 5))\n\nplt.subplot(1, 2, 1)\nplt.imshow(images[0].permute(1, 2, 0).cpu())  # Permute dimensions to show the RGB image correctly\nplt.title(\"Image 1\")\nplt.axis(\"off\")\n\nplt.subplot(1, 2, 2)\nplt.imshow(masks[0].permute(1, 2, 0).cpu(), cmap='gray')  # Use gray colormap for masks\nplt.title(\"Mask 1\")\nplt.axis(\"off\")\n\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-04-21T07:17:39.397118Z","iopub.execute_input":"2024-04-21T07:17:39.397445Z","iopub.status.idle":"2024-04-21T07:17:44.793169Z","shell.execute_reply.started":"2024-04-21T07:17:39.397418Z","shell.execute_reply":"2024-04-21T07:17:44.792079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Test","metadata":{}},{"cell_type":"code","source":"test_img_dir = '/kaggle/input/camvid/CamVid/test'\ntest_mask_dir = '/kaggle/input/camvid/CamVid/test_labels'\nclass_dict_path = '/kaggle/input/camvid/CamVid/class_dict.csv'\n\ntest_dataset  = CamVidDataset(test_img_dir, test_mask_dir, class_dict_path, transform=transform, mask_transform = mask_transform)\n\ntest_loader = DataLoader(test_dataset, batch_size=16, shuffle=True)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-21T10:35:28.178940Z","iopub.execute_input":"2024-04-21T10:35:28.179685Z","iopub.status.idle":"2024-04-21T10:35:28.373939Z","shell.execute_reply.started":"2024-04-21T10:35:28.179644Z","shell.execute_reply":"2024-04-21T10:35:28.372848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataset.class_name_to_int","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_loader_iterator = iter(test_loader)\n\ntest_images, test_masks = next(test_loader_iterator)\n\nprint(\"Image batch shape:\", test_images.shape)\nprint(\"Mask batch shape:\", test_masks.shape)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-21T10:35:31.080129Z","iopub.execute_input":"2024-04-21T10:35:31.080898Z","iopub.status.idle":"2024-04-21T10:35:36.184553Z","shell.execute_reply.started":"2024-04-21T10:35:31.080859Z","shell.execute_reply":"2024-04-21T10:35:36.183494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_loader_iterator = iter(test_loader)\ntest_images, test_masks = next(test_loader_iterator)\n\n\nplt.figure(figsize=(10, 5))\n\nplt.subplot(1, 2, 1)\nplt.imshow(test_images[0].permute(1, 2, 0).cpu())  # Permute dimensions to show the RGB image correctly\nplt.title(\"Image 1\")\nplt.axis(\"off\")\n\nplt.subplot(1, 2, 2)\nplt.imshow(test_masks[0].permute(1, 2, 0).cpu(), cmap='gray')  # Use gray colormap for masks\nplt.title(\"Mask 1\")\nplt.axis(\"off\")\n\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-04-21T07:17:49.955131Z","iopub.execute_input":"2024-04-21T07:17:49.955807Z","iopub.status.idle":"2024-04-21T07:17:55.322810Z","shell.execute_reply.started":"2024-04-21T07:17:49.955768Z","shell.execute_reply":"2024-04-21T07:17:55.321741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Val","metadata":{}},{"cell_type":"code","source":"val_img_dir = '/kaggle/input/camvid/CamVid/val'\nval_mask_dir = '/kaggle/input/camvid/CamVid/val_labels'\nclass_dict_path = '/kaggle/input/camvid/CamVid/class_dict.csv'\n\nval_dataset  = CamVidDataset(val_img_dir, val_mask_dir, class_dict_path, transform=transform, mask_transform = mask_transform)\n\nval_loader = DataLoader(val_dataset, batch_size=16, shuffle=True)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-21T10:35:36.186226Z","iopub.execute_input":"2024-04-21T10:35:36.186540Z","iopub.status.idle":"2024-04-21T10:35:36.288070Z","shell.execute_reply.started":"2024-04-21T10:35:36.186514Z","shell.execute_reply":"2024-04-21T10:35:36.287339Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_loader_iterator = iter(val_loader)\n\nval_images, val_masks = next(val_loader_iterator)\n\nprint(\"Image batch shape:\", val_images.shape)\nprint(\"Mask batch shape:\", val_masks.shape)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-21T10:35:36.289339Z","iopub.execute_input":"2024-04-21T10:35:36.289810Z","iopub.status.idle":"2024-04-21T10:35:41.033606Z","shell.execute_reply.started":"2024-04-21T10:35:36.289777Z","shell.execute_reply":"2024-04-21T10:35:41.032655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Sample one batch from the train loader\nval_loader_iterator = iter(val_loader)\nval_images, val_masks = next(val_loader_iterator)\n\n\n# Display one image and its mask\nplt.figure(figsize=(10, 5))\n\nplt.subplot(1, 2, 1)\nplt.imshow(val_images[0].permute(1, 2, 0).cpu())  # Permute dimensions to show the RGB image correctly\nplt.title(\"Image 1\")\nplt.axis(\"off\")\n\nplt.subplot(1, 2, 2)\nplt.imshow(val_masks[0].permute(1, 2, 0).cpu(), cmap='gray')  # Use gray colormap for masks\nplt.title(\"Mask 1\")\nplt.axis(\"off\")\n\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-04-21T07:18:00.587978Z","iopub.execute_input":"2024-04-21T07:18:00.588271Z","iopub.status.idle":"2024-04-21T07:18:06.039388Z","shell.execute_reply.started":"2024-04-21T07:18:00.588243Z","shell.execute_reply":"2024-04-21T07:18:06.038479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training loop , Testing","metadata":{}},{"cell_type":"markdown","source":"## Quantization-aware training","metadata":{}},{"cell_type":"code","source":"from torch.quantization import prepare_qat, convert, get_default_qat_qconfig\n\n\nmodel = UNet(n_class=32)\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n\nmodel_fp32 = model.to('cpu')\n\n\nqconfig = get_default_qat_qconfig('qnnpack')\nmodel_fp32.qconfig = qconfig\n\n\nmodel_fp32 = prepare_qat(model_fp32, inplace=True)\n\n\ncriterion = nn.CrossEntropyLoss()\n\n\nnum_epochs = 30\nfor epoch in range(num_epochs):\n    model_fp32.train()\n    running_loss = 0.0\n    \n    for images, masks in train_loader:\n\n        images, masks = images.to('cpu'), masks.to('cpu')\n        \n        masks = masks.squeeze(1)\n        \n        optimizer.zero_grad()\n        \n        outputs = model_fp32(images)\n        loss = criterion(outputs, masks)\n        \n        loss.backward()\n        optimizer.step()\n        \n        running_loss += loss.item()\n    \n    epoch_loss = running_loss / len(train_loader)\n    \n    model_fp32.eval()\n    val_running_loss = 0.0\n    \n    with torch.no_grad():\n        for val_images, val_masks in val_loader:\n\n            val_images, val_masks = val_images.to('cpu'), val_masks.to('cpu')\n            \n            val_masks = val_masks.squeeze(1)\n            \n            val_outputs = model_fp32(val_images)\n            \n            val_loss = criterion(val_outputs, val_masks)\n            \n            val_running_loss += val_loss.item()\n        \n    val_epoch_loss = val_running_loss / len(val_loader)\n    \n    print(f\"Epoch {epoch + 1}/{num_epochs}, Training Loss: {epoch_loss:.4f}, Validation Loss: {val_epoch_loss:.4f}\")\n\n# Convert the QAT-trained model to a fully quantized model\nmodel_int8 = model_fp32.to('cpu')  # Moving the QAT model to CPU\nmodel_int8 = convert(model_int8, inplace=True)  # Convert to a quantized model\n","metadata":{"execution":{"iopub.status.busy":"2024-04-23T12:28:22.179396Z","iopub.execute_input":"2024-04-23T12:28:22.180596Z","iopub.status.idle":"2024-04-23T12:28:22.309994Z","shell.execute_reply.started":"2024-04-23T12:28:22.180541Z","shell.execute_reply":"2024-04-23T12:28:22.308848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_int8 = model_int8.to('cpu')\nmodel_int8.eval()\n\n\ntotal_accuracy = 0\ntotal_iou = 0\nnum_batches = 0\n\nwith torch.no_grad():\n    for images, masks in test_loader:\n\n        images, masks = images.to('cpu'), masks.to('cpu')\n\n        images = images.to(torch.float32)\n\n        outputs = model_int8(images)\n\n        _, predicted_classes = torch.max(outputs, dim=1)\n\n        intersection = (predicted_classes & masks).sum(dim=(1, 2))\n        union = (predicted_classes | masks).sum(dim=(1, 2))\n        iou = (intersection / union).mean().item()\n\n        correct = (predicted_classes == masks).float().mean(dim=(1, 2))\n        accuracy = correct.mean().item()\n\n        total_iou += iou\n        total_accuracy += accuracy\n        num_batches += 1\n\naverage_iou = total_iou / num_batches\naverage_accuracy = total_accuracy / num_batches\n\nprint(f\"Average IoU: {average_iou:.4f}\")\nprint(f\"Average Accuracy: {average_accuracy:.4f}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-04-21T14:04:21.396223Z","iopub.execute_input":"2024-04-21T14:04:21.397112Z","iopub.status.idle":"2024-04-21T14:05:34.326610Z","shell.execute_reply.started":"2024-04-21T14:04:21.397078Z","shell.execute_reply":"2024-04-21T14:05:34.325858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Output masks","metadata":{}},{"cell_type":"code","source":"model_int8.eval()\nmodel_int8 = model_int8.to('cpu')\n\ntest_loader_iterator = iter(test_loader)\nimages, masks = next(test_loader_iterator)\n\nimages, masks = images.to('cpu'), masks.to('cpu')\n\nimages = images.to(torch.float32) \nmasks = masks.to(torch.uint8) \n\nwith torch.no_grad():\n    outputs = model_int8(images)\n\n_, predicted_classes = torch.max(outputs, dim=1)\n\nnum_plots = 4\n\nplt.figure(figsize=(15, 10))\n\n\nfor i in range(num_plots):\n\n    plt.subplot(3, num_plots, i + 1)\n    plt.imshow(images[i].permute(1, 2, 0).cpu().numpy())  # Convert image shape from (C, H, W) to (H, W, C)\n    plt.title(f\"Input Image {i + 1}\")\n    plt.axis(\"off\")\n\n\n    plt.subplot(3, num_plots, i + num_plots + 1)\n    plt.imshow(predicted_classes[i].cpu().numpy(), cmap='gray')\n    plt.title(f\"Predicted Mask {i + 1}\")\n    plt.axis(\"off\")\n\n\n    plt.subplot(3, num_plots, i + 2 * num_plots + 1)\n    plt.imshow(masks[i].permute(1,2,0).cpu().numpy(), cmap='gray')\n    plt.title(f\"Ground Truth Mask {i + 1}\")\n    plt.axis(\"off\")\n\nplt.tight_layout()\n\nplt.show()\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"file_path = 'test_3.txt'\n\nnp.set_printoptions(threshold=np.inf, precision=6)\n\nwith open(file_path, 'w') as f:\n\n    for name, module in model_int8.named_modules():\n\n        if isinstance(module, torch.ao.nn.quantized.modules.conv.Conv2d):\n\n            packed_params = module._packed_params\n            \n            weights, biases = packed_params.unpack()\n            \n            f.write(f\"Module name: {name}\\n\")\n            \n            dequantized_weights = torch.dequantize(weights)\n            \n            weights_np = dequantized_weights.detach().numpy()\n            f.write(f\"Weights:\\n{weights_np}\\n\")\n            \n            if biases is not None:\n                dequantized_biases = torch.dequantize(biases)\n                biases_np = dequantized_biases.detach().numpy()\n                f.write(f\"Biases:\\n{biases_np}\\n\")\n            else:\n                f.write(\"No biases\\n\")\n\nprint(f\"Model parameters have been written to {file_path}.\")\n","metadata":{"execution":{"iopub.status.busy":"2024-04-23T12:33:22.883955Z","iopub.execute_input":"2024-04-23T12:33:22.884429Z","iopub.status.idle":"2024-04-23T12:33:26.812537Z","shell.execute_reply.started":"2024-04-23T12:33:22.884397Z","shell.execute_reply":"2024-04-23T12:33:26.811347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for name, module in model_int8.named_modules():\n    \n    if isinstance(module, torch.ao.nn.quantized.modules.conv.Conv2d):\n        \n        packed_params = module._packed_params\n\n        weights, biases = packed_params.unpack()\n        \n        param_np = weights.detach().cpu()\n\n        param_min = param_np.min()\n        param_max = param_np.max()\n\n        print(f\"Parameter Name: {name}, Min: {param_min}, Max: {param_max}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-04-21T14:29:50.406171Z","iopub.execute_input":"2024-04-21T14:29:50.406950Z","iopub.status.idle":"2024-04-21T14:29:50.418273Z","shell.execute_reply.started":"2024-04-21T14:29:50.406918Z","shell.execute_reply":"2024-04-21T14:29:50.417233Z"},"trusted":true},"execution_count":null,"outputs":[]}]}